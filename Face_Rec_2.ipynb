{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face Rec#2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gvpx3igRL4r"
      },
      "source": [
        "import cv2\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x5VB1XBRRTz"
      },
      "source": [
        "main_dir = os.getcwd()\n",
        "main_dir = os.path.join(main_dir, 'drive')\n",
        "main_dir = os.path.join(main_dir, 'MyDrive')\n",
        "main_dir = os.path.join(main_dir, 'Colab Notebooks')\n",
        "\n",
        "train_dir = os.path.join(main_dir, 'tr')\n",
        "test_dir = os.path.join(main_dir, 'te')\n",
        "val_dir = os.path.join(main_dir, 'va')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu8cRzAoXZsY"
      },
      "source": [
        "from keras.applications import vgg16"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEr5GCSxXvZ7",
        "outputId": "6d87566f-fa24-45d0-e004-66ed9a731515"
      },
      "source": [
        "img_rows, img_cols = 224, 224\n",
        "\n",
        "model = vgg16.VGG16(weights = 'imagenet',\n",
        "                    include_top = False,\n",
        "                    input_shape = (img_rows, img_cols, 3))\n",
        "\n",
        "for layer in model.layers:\n",
        "  layer.trainable = False"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C75RIqL6YNy7"
      },
      "source": [
        "def layer_adder(bottom_model, num_classes):\n",
        "  top_model = bottom_model.output\n",
        "  top_model = GlobalAveragePooling2D()(top_model)\n",
        "  top_model = Dense(1024, activation = 'relu')(top_model)\n",
        "  top_model = Dense(512, activation = 'relu')(top_model)\n",
        "  top_model = Dense(num_classes, activation = 'softmax')(top_model)\n",
        "  return top_model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7jvQWiTZtgj",
        "outputId": "a2d712fe-cf87-4deb-a23e-05f91f20b6b6"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Model\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "FC_head = layer_adder(model, num_classes)\n",
        "\n",
        "model = Model(inputs = model.input, outputs = FC_head)\n",
        "\n",
        "print(model.summary)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Model.summary of <keras.engine.functional.Functional object at 0x7f507e3246d0>>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQvlQHNndt-M",
        "outputId": "ade9818d-c26e-41ec-cea4-d8c8be938b4a"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size = (img_rows, img_cols),\n",
        "    batch_size = batch_size,\n",
        "    class_mode = 'categorical')\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size = (img_rows, img_cols),\n",
        "    batch_size = batch_size,\n",
        "    class_mode = 'categorical')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1174 images belonging to 10 classes.\n",
            "Found 361 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Jb1SdTIZd1P",
        "outputId": "990b5bee-1a71-4b8a-d07e-e6e8dba7ef68"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"face_detector.h15\",\n",
        "                             monitor = \"val_loss\",\n",
        "                             mode = \"min\",\n",
        "                             save_best_only = True,\n",
        "                             verbose = 1)\n",
        "\n",
        "earlystop = EarlyStopping(monitor = \"val_loss\",\n",
        "                          min_delta = 0,\n",
        "                          patience = 3,\n",
        "                          verbose = 1,\n",
        "                          restore_best_weights = True)\n",
        "\n",
        "callbacks = [earlystop, checkpoint]\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = 'Adam',\n",
        "              metrics = 'accuracy')\n",
        "\n",
        "nb_train_samples = 1174 \n",
        "nb_val_samples = 361 \n",
        "\n",
        "epochs = 10\n",
        "batch_size = 16\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    epochs = epochs,\n",
        "    callbacks = callbacks,\n",
        "    validation_data = val_generator,\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1915: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "74/74 [==============================] - 463s 6s/step - loss: 2.2515 - accuracy: 0.1514 - val_loss: 1.7559 - val_accuracy: 0.3380\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.75593, saving model to face_detector.h15\n",
            "INFO:tensorflow:Assets written to: face_detector.h15/assets\n",
            "Epoch 2/10\n",
            "74/74 [==============================] - 6s 86ms/step - loss: 1.6671 - accuracy: 0.4002 - val_loss: 1.4317 - val_accuracy: 0.5125\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.75593 to 1.43165, saving model to face_detector.h15\n",
            "INFO:tensorflow:Assets written to: face_detector.h15/assets\n",
            "Epoch 3/10\n",
            "74/74 [==============================] - 6s 86ms/step - loss: 1.3821 - accuracy: 0.5075 - val_loss: 1.3449 - val_accuracy: 0.5125\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.43165 to 1.34486, saving model to face_detector.h15\n",
            "INFO:tensorflow:Assets written to: face_detector.h15/assets\n",
            "Epoch 4/10\n",
            "74/74 [==============================] - 6s 85ms/step - loss: 1.2122 - accuracy: 0.5420 - val_loss: 1.1840 - val_accuracy: 0.5928\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.34486 to 1.18398, saving model to face_detector.h15\n",
            "INFO:tensorflow:Assets written to: face_detector.h15/assets\n",
            "Epoch 5/10\n",
            "74/74 [==============================] - 6s 85ms/step - loss: 1.0502 - accuracy: 0.6146 - val_loss: 1.1077 - val_accuracy: 0.6094\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.18398 to 1.10768, saving model to face_detector.h15\n",
            "INFO:tensorflow:Assets written to: face_detector.h15/assets\n",
            "Epoch 6/10\n",
            "74/74 [==============================] - 6s 85ms/step - loss: 0.8522 - accuracy: 0.7026 - val_loss: 1.1596 - val_accuracy: 0.6316\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.10768\n",
            "Epoch 7/10\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 0.8240 - accuracy: 0.7122 - val_loss: 1.0514 - val_accuracy: 0.6343\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.10768 to 1.05138, saving model to face_detector.h15\n",
            "INFO:tensorflow:Assets written to: face_detector.h15/assets\n",
            "Epoch 8/10\n",
            "74/74 [==============================] - 6s 85ms/step - loss: 0.6731 - accuracy: 0.7617 - val_loss: 0.9506 - val_accuracy: 0.6454\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.05138 to 0.95057, saving model to face_detector.h15\n",
            "INFO:tensorflow:Assets written to: face_detector.h15/assets\n",
            "Epoch 9/10\n",
            "74/74 [==============================] - 6s 86ms/step - loss: 0.6147 - accuracy: 0.8027 - val_loss: 0.9455 - val_accuracy: 0.6925\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.95057 to 0.94554, saving model to face_detector.h15\n",
            "INFO:tensorflow:Assets written to: face_detector.h15/assets\n",
            "Epoch 10/10\n",
            "74/74 [==============================] - 7s 88ms/step - loss: 0.5977 - accuracy: 0.7918 - val_loss: 0.9680 - val_accuracy: 0.6787\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.94554\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}